\subsection{Preprocessing}

\subsubsection{Signal Synchronization and Quality Control}
To match biosignal recordings with reference data like EEG or video, many studies performed time synchronization. This included adjusting for time drift between devices using start and end timestamps or using the network time protocol (NTP) \cite{Onorati2017-bn,Vakilna2024-hk,Tang2021-td}. Some studies checked signal quality and removed invalid parts of the data. For example, segments were excluded if the temperature was too low or too high (less than 27°C or more than 45°C), which indicated the device was not worn properly \cite{Yu2023-ss,Tang2021-td}. Other signals were removed based on quality checks, like low EDA amplitude or poor PPG quality \cite{Ge2023-ab,Arends2018-ew}. Also, the first and last 15 minutes of recordings were sometimes discarded to avoid artifacts from calibration \cite{Yu2023-ss}.

\subsubsection{Noise and Artifact Removal}
Most studies used filtering to reduce unwanted signals caused by movement, other physiological activity, or environmental noise. Bandpass filters were common. For example, accelerometer and gyroscope signals were filtered between 0.2–47 Hz \cite{Milosevic2016-ee,De_Cooman2018-pq}, 1–24 Hz \cite{Wu2024-yl}, or 0.5–35 Hz \cite{Gheryani2017-yg}. EMG signals were filtered between 20–90 Hz \cite{Wu2024-yl} or with a high-pass filter at 20 Hz \cite{Milosevic2016-ee,De_Cooman2018-pq}. ECG signals used a notch filter at 60 Hz to remove electrical interference from power lines \cite{Hamlin2021-sd}. Some studies smoothed signals using moving averages, like a 10-minute average for temperature \cite{Yu2023-ss} or a 15-point average for EDA \cite{Wang2022-lt}. Median filters were also used for EDA signals \cite{Wang2025-ql}. A few studies used wavelet transforms to break down signals like ECG-derived respiration (EDR) to level 7 to improve entropy \cite{Forooghifar2022-dm}, or to clean heart rate and GSR signals \cite{Jiang2022-zu}.

\subsubsection{Data Segmentation and Windowing}
To analyze the signals, the data was divided into short or long windows. The window length depended on the type of seizure being studied. Short windows, between 2 and 10 seconds, were used to detect convulsive seizures. For example, one study used 2-second windows with 75\% overlap for ACC and EMG data \cite{Milosevic2016-ee,De_Cooman2018-pq,Poh2012-af}. Longer windows, from 1 to 7 minutes, were used to detect slower changes in the body, such as with PPG, EDA \cite{Ramirez-Peralta2021-nq}, HRV \cite{Jiang2022-zu}, or for detecting generalized convulsive seizures (GCS) \cite{Vakilna2024-hk}. Some studies removed low-motion periods by checking if acceleration was too low, such as standard deviation below 0.2g \cite{Wang2022-lt,Dong2022-oo}, or only analyzed data when acceleration was above 0.1g \cite{Larsen2024-vn}.

\subsubsection{Class Imbalance Handling}
Since seizures are rare compared to non-seizure events, many studies dealt with this class imbalance. One method was undersampling, where non-seizure data was randomly removed to create a more balanced dataset. For example, one study used a seizure-to-nonseizure ratio of 1:1.5 \cite{Yu2023-ss,Tang2021-td}. Other studies used oversampling, where seizure data was repeated to make it more balanced, especially for short seizures \cite{Larsen2024-vn}. Some used automatic filtering to remove non-seizure data based on certain signal patterns, like when the main frequency of acceleration was below 2 Hz or when the signal had a noncross ratio above 0.9 \cite{Wang2022-lt}.

\subsubsection{Features Extraction}
Most studies created features from the signals to use in machine learning models. Time-domain features included basic statistics like mean, variance, and entropy \cite{Wang2022-lt,Dong2022-oo}, zero-crossing rates \cite{De_Cooman2018-pq}, and counts of local maxima \cite{Milosevic2016-ee}. Frequency-domain features included power in certain frequency bands, like 9–22.5 Hz \cite{Milosevic2016-ee}, FFT peaks \cite{Wang2022-lt}, and heart rate variability from Lomb-Scargle analysis \cite{Ramirez-Peralta2021-nq}. Some studies combined features from different sensors, such as ACC and EDA \cite{Regalia2019-ch,Wu2024-yl}, or used decision-level fusion \cite{Chowdhury2022-bi}. To reduce the number of features and keep only the most useful ones, they used methods like minimum redundancy maximum relevance (mRMR) \cite{Wang2022-lt,Ge2023-ab}, ANOVA \cite{Dong2022-oo}, or the Wilcoxon rank-sum test \cite{Vakilna2024-hk}.

\subsubsection{Normalization and Baseline Correction}
Normalization helped make signals more comparable between different people or recording sessions. Some studies used z-score normalization, which adjusts signals to have a mean of zero and a standard deviation of one \cite{Nasseri2021-xn}. Others used moving baselines, where the recent history of a signal (such as over 60 seconds) was used to calculate thresholds for heart rate or oxygen saturation \cite{Cogan2015-lu}. Personalized baselines were also used, where each subject’s median signal was used as a reference point \cite{Jiang2022-zu}.
