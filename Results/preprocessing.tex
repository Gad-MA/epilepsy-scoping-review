\subsection{Preprocessing}

\subsubsection{Signal Synchronization and Quality Control}
A fundamental preprocessing step reported in the literature was the temporal alignment of data from wearable devices with a ground-truth reference, typically video electroencephalography (video-EEG), to ensure accurate event labeling. Four studies described specific synchronization methods \cite{Yu2023-ss, Vakilna2024-hk, Arends2018-ew, Tang2021-td}. The reported techniques included manual clock synchronization at the start and end of recordings to correct for time drift \cite{Yu2023-ss}, the use of a Network Time Protocol (NTP) for automated alignment \cite{Vakilna2024-hk}, a three-step timing error compensation algorithm \cite{Tang2021-td}, and the synchronization of sensor data with event logs from video recordings \cite{Arends2018-ew}.

Following synchronization, quality control was performed to ensure data integrity. Ten studies detailed their quality control procedures \cite{Yu2023-ss, Li2022-ty, Arends2018-ew, Jiang2022-zu, Wang2025-my, Nasseri2021-xn, Tang2021-td, Ali2020-ke, Ge2023-ab, Hegarty-Craver2021-hk}. A common strategy was the removal of data segments where the device was not worn. Two studies accomplished this using temperature data, excluding periods where sensed temperature fell outside a physiological range \cite{Yu2023-ss, Tang2021-td}. Other methods involved trimming the initial and final 15 minutes of recordings to account for sensor calibration \cite{Yu2023-ss} or implementing signal-specific quality checks. For instance, heart rate data were only utilized if a signal quality index exceeded an 80\% threshold \cite{Arends2018-ew}, while data from segments with poor electrodermal activity (EDA) contact were discarded \cite{Li2022-ty}. Other studies also reported using custom quality indices to clean data epochs before analysis \cite{Nasseri2021-xn, Wang2025-my}.

Furthermore, some protocols involved removing sliding windows with a high percentage ($>$60\%) of missing data \cite{Jiang2022-zu} or conducting manual reviews to identify and exclude recordings with failures or invalid annotations \cite{Yu2023-ss}. General artifact removal and signal cleaning were also explicitly mentioned in several other papers \cite{Ge2023-ab, Hegarty-Craver2021-hk, Ali2020-ke}.


\subsubsection{Noise and Artifact Removal}
Majority of the studies (16 papers) reported applying specific signal processing techniques to remove noise and artifacts from the raw sensor data \cite{Milosevic2016-ee, De_Cooman2018-pq, Wu2024-yl, Li2022-ty, Wang2025-my, Hamlin2021-sd, Chowdhury2022-bi, Gheryani2017-yg, Ge2023-ab, Xu2022-tx, Wang2025-ql, Jiang2022-zu, Yu2023-ss, Dong2022-oo, Vakilna2024-hk, Hegarty-Craver2021-hk}. Filtering was the most common approach, with techniques tailored to the specific signal modality. For accelerometer (ACC) and gyroscope (Gyro) data, band-pass filters were frequently used, with cutoff frequencies such as 1–24 Hz \cite{Wu2024-yl} or 0.2–47 Hz \cite{De_Cooman2018-pq}, to isolate movement patterns relevant to seizures. High-pass filters were also applied to remove baseline drift or focus on significant movements \cite{Milosevic2016-ee, Li2022-ty, Vakilna2024-hk, Wang2025-my}. In addition to filtering, smoothing techniques like a five-point median filter \cite{Xu2022-tx} or a 3-second smoothing filter \cite{Cogan2017-lg} were employed to reduce erratic signal fluctuations.

For other modalities, filtering strategies were similarly targeted. Surface electromyography (sEMG) signals were commonly processed with a high-pass filter (e.g., at 20 Hz) to remove motion artifacts and baseline noise \cite{Milosevic2016-ee, De_Cooman2018-pq, Li2022-ty} or a band-pass filter (e.g., 20-90 Hz) \cite{Wu2024-yl}. Electrodermal activity (EDA) signals were often smoothed using a mean or median filter to reduce noise \cite{Li2022-ty, Wang2025-ql, Yu2023-ss}. Furthermore, a study reported the use of wavelet transformation for denoising multiple signal types, including heart rate (HR), EDA, and ACC \cite{Jiang2022-zu}, while others used notch filters to eliminate specific sources of interference like powerline noise \cite{Milosevic2016-ee, Hamlin2021-sd}.
In the context of seizure prediction and forecasting, noise removal was also a noted preprocessing step. One study focusing on seizure prediction applied low-pass filtering specifically to the EDA signal to refine the data before analysis \cite{Vieluf2023-ta}. This highlights that while the ultimate goal (detection vs. prediction) differs, the foundational need to clean and denoise raw sensor signals remains a consistent and critical practice across the field.


\subsubsection{Data Segmentation and Windowing}
A nearly universal preprocessing step reported in the literature was the segmentation of continuous time-series data into fixed-length windows, a necessary step for feature extraction in most machine learning models. This technique was detailed in 18 of the detection studies \cite{Milosevic2016-ee, De_Cooman2018-pq, Cogan2017-lg, Hamlin2021-sd, Wang2022-lt, Poh2012-af, Wu2024-yl, Nasseri2021-xn, Vakilna2024-hk, Larsen2024-vn, Dong2022-oo, Li2022-ty, Xu2022-tx, Wang2025-ql, Jiang2022-zu, Wang2025-my, Hegarty-Craver2021-hk, Chowdhury2022-bi} and was also a key component of all of the 3 forecasting and prediction studies \cite{Meisel2020-ii, Vieluf2023-zv, Vieluf2023-ta}.

The duration of these windows varied considerably, often tailored to the physiological signals being analyzed. For capturing the rapid motor patterns characteristic of tonic-clonic seizures, shorter window sizes ranging from 2 to 6 seconds were frequently employed \cite{Milosevic2016-ee, De_Cooman2018-pq, Dong2022-oo, Cogan2017-lg, Hamlin2021-sd, Wu2024-yl, Xu2022-tx, Wang2025-ql, Wang2025-my}. A 10-second window was also a common choice \cite{Poh2012-af, Nasseri2021-xn, Larsen2024-vn, Li2022-ty}. While two studies used much larger segments, ranging from 5 to 7 minutes \cite{Jiang2022-zu, Vakilna2024-hk}.

To ensure that seizure events occurring at the boundary of a window were not missed and to augment the volume of training data, studies implemented overlapping windows. The degree of overlap typically ranged from 50\% \cite{Hamlin2021-sd, Dong2022-oo, Wang2025-ql, Vakilna2024-hk} to as high as 90\% \cite{Wang2022-lt, Larsen2024-vn}. Overlaps of 75-80\% were also frequently reported \cite{Milosevic2016-ee, De_Cooman2018-pq, Cogan2017-lg, Poh2012-af, Wu2024-yl, Jiang2022-zu}.

All reviewed forecasting and prediction studies used data preprocessing segmentation; however, segmentation methods varied across the studies. The segmentation strategies ranged from short, fixed-length epochs (30 seconds) and brief daily snapshots (15 minutes) to longer, seizure-centered windows (45 minutes). For example, one study \cite{Meisel2020-ii} segmented continuous wristband sensor data into 30-second epochs, extracting features from each segment to train and evaluate their seizure forecasting model. Another study \cite{Vieluf2023-zv} utilized a single 15-minute autonomic recording taken in the evening (9:00–9:15 pm) as a fixed snapshot for seizure likelihood assessment, combining this with clinical data for prediction. In contrast, \cite{Vieluf2023-ta} applied segmentation of 45-minute preictal and interictal periods based on EEG seizure annotations, focusing on capturing nonlinear correlations between EDA and HR signals for unsupervised seizure prediction.


\subsubsection{Class Imbalance Handling}
A significant challenge inherent in seizure detection is the profound class imbalance between rare seizure events and the vast amount of non-seizure data. Seven of the reviewed detection studies explicitly reported strategies to address this issue during model training \cite{Yu2023-ss, Wang2022-lt, Wu2024-yl, Nasseri2021-xn, Vakilna2024-hk, Larsen2024-vn, Tang2021-td}. No such methods were detailed in the provided papers on seizure forecasting or prediction.

The most frequently cited technique was undersampling, where the number of non-seizure samples is reduced to create a more balanced dataset. Two studies employed random undersampling of non-seizure segments \cite{Yu2023-ss, Tang2021-td}. A similar approach involved subsampling non-seizure epochs to achieve a specific, more manageable seizure-to-non-seizure ratio of 1:10 for training \cite{Vakilna2024-hk}.

Conversely, other studies used oversampling or data augmentation techniques. One paper reported using oversampling for seizure events, in combination with algorithmic class weighting, to increase the influence of the minority class \cite{Larsen2024-vn}. Another study implemented a novel approach by applying a much higher (90\%) overlap in its sliding window segmentation for seizure data, which effectively increased the number of seizure-labeled samples for training achieving seizure-to-non-seizure ratio of 1:1.5 \cite{Wang2022-lt}. These techniques are crucial for preventing machine learning models from developing a bias towards the majority (non-seizure) class and thereby failing to detect true seizure events.


\subsubsection{Features Extraction}
The extraction of features from the windowed signal data is a critical step. This process transforms the raw, time-series signals into a structured format suitable for detection algorithms. Feature extraction was explicitly detailed 17 of the detection studies \cite{Milosevic2016-ee, De_Cooman2018-pq, Hamlin2021-sd, Wang2022-lt, Poh2012-af, Wu2024-yl, Chowdhury2022-bi, Gheryani2017-yg, Ge2023-ab, Vakilna2024-hk, Larsen2024-vn, Dong2022-oo, Li2022-ty, Xu2022-tx, Wang2025-ql, Regalia2019-ch, Wang2025-my} and was central to the methodologies of all three forecasting and prediction papers \cite{Meisel2020-ii, Vieluf2023-zv, Vieluf2023-ta}.

The extracted features spanned multiple domains. The most common were statistical and time-domain features, including measures like mean, variance, interquartile range, and the number of local maxima or zero-crossings \cite{Milosevic2016-ee, De_Cooman2018-pq, Dong2022-oo, Wang2025-my}. Frequency-domain features, derived from techniques like the Fourier transform or wavelet transforms, were also widely used \cite{Poh2012-af, Chowdhury2022-bi, De_Cooman2018-pq, Vakilna2024-hk}. The number of features extracted varied significantly, from 19 in one study \cite{Poh2012-af} to as many as 594 in another \cite{Larsen2024-vn}. This process often involved deriving features from novel signals, such as attitude angles (pitch and roll), which were shown to be effective proxies or replacements for traditional accelerometer and gyroscope data \cite{Wang2025-ql}.

Given the high dimensionality of the feature sets, several studies employed feature selection or dimensionality reduction techniques to identify the most salient predictors and reduce computational complexity. The methods reported include Principal Component Analysis (PCA) \cite{Chowdhury2022-bi}, statistical tests like ANOVA \cite{Dong2022-oo}, T-tests \cite{Ge2023-ab}, and the Wilcoxon rank-sum test \cite{Vakilna2024-hk}, as well as machine learning-based approaches like Random Forest feature selection \cite{Xu2022-tx} and minimum Redundancy Maximum Relevance (mRMR) \cite{Ge2023-ab}. These techniques were shown to optimize the feature set, for instance by reducing 91 initial features to 71 \cite{Xu2022-tx}, and ultimately improve classification performance \cite{Ge2023-ab}.

In the context of forecasting and prediction, feature extraction was similarly vital but sometimes focused on different aspects of the physiological data. One study calculated Heart Rate Variability (HRV) metrics, specifically the root mean square of successive differences (RMSSD), to assess impending seizure likelihood \cite{Vieluf2023-zv}. Another forecasting approach extracted features from not only the raw signals and their Fourier transforms but also incorporated non-physiological data such as signal quality indices and the time of day, creating a rich, 17-channel input for its model \cite{Meisel2020-ii}.


\subsubsection{Normalization and Baseline Correction}
To account for inter-patient physiological variability and to standardize the input for detection algorithms, several studies incorporated normalization or baseline correction steps. This process was detailed in seven of the detection-focused papers \cite{Nasseri2021-xn, Jiang2022-zu, Hegarty-Craver2021-hk, Cogan2017-lg, Ge2023-ab, Milosevic2016-ee, Wang2025-ql}.

Different methodologies were employed to establish and correct for a patient's physiological baseline. A common technique was the use of a moving baseline, where incoming data from a short window (e.g., 5 seconds) were compared against a constantly updated reference to detect sudden deviations indicative of a seizure \cite{Cogan2017-lg}. A more personalized approach involved defining subject-specific baselines, for example, by using the median of a patient's signals over a period, and then calculating residuals from this baseline for analysis \cite{Jiang2022-zu}. Other studies mentioned baseline correction or calibration as a general step for signals like ACC and GYRO \cite{Ge2023-ab, Wang2025-ql}, or used high-pass filtering as an implicit method to remove baseline drift from ACC signals \cite{Milosevic2016-ee}.

In addition to baseline correction, signal normalization or standardization was also performed. Normalization helped make signals more comparable between different people or recording sessions. Some studies used z-score normalization, which adjusts signals to have a mean of zero and a standard deviation of one \cite{Nasseri2021-xn, Hegarty-Craver2021-hk}. These procedures ensure that the scale of different signals and features does not unduly influence the model's performance and help in adapting the algorithm to individual physiological characteristics.
